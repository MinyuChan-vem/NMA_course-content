{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MinyuChan-vem/NMA_course-content/blob/main/HCP_language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The HCP dataset comprises task-based fMRI from a large sample of human subjects. The NMA-curated dataset includes time series data that has been preprocessed and spatially-downsampled by aggregating within 360 regions of interest.\n",
        "\n",
        "In order to use this dataset, please electronically sign the HCP data use terms at ConnectomeDB. Instructions for this are on pp. 24-25 of the HCP Reference Manual.\n",
        "\n",
        "In this notebook, NMA provides code for downloading the data and doing some basic visualisation and processing.\n",
        "\n",
        "For a detailed description of the tasks have a look pages 45-54 of the HCP reference manual."
      ],
      "metadata": {
        "id": "YWiKbDJV1SvQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3b2LS-y8ya3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10c63ea5-3a02-4811-a3d7-d3c217a1ebd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# @title Install dependencies\n",
        "!pip install nilearn --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "gsRZbAWJzDqp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Figure settings\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
      ],
      "metadata": {
        "id": "2YrxECrIzU0y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The data shared for NMA projects is a subset of the full HCP dataset\n",
        "N_SUBJECTS = 100\n",
        "\n",
        "# The data have already been aggregated into ROIs from the Glasser parcellation\n",
        "N_PARCELS = 360\n",
        "\n",
        "# The acquisition parameters for all tasks were identical\n",
        "TR = 0.72  # Time resolution, in seconds\n",
        "\n",
        "# The parcels are matched across hemispheres with the same order\n",
        "HEMIS = [\"Right\", \"Left\"]\n",
        "\n",
        "# Each experiment was repeated twice in each subject\n",
        "RUNS   = ['LR','RL']\n",
        "N_RUNS = 2\n",
        "\n",
        "# There are 7 tasks. Each has a number of 'conditions'\n",
        "# TIP: look inside the data folders for more fine-graned conditions\n",
        "\n",
        "EXPERIMENTS = {\n",
        "    'MOTOR'      : {'cond':['lf','rf','lh','rh','t','cue']},\n",
        "    'WM'         : {'cond':['0bk_body','0bk_faces','0bk_places','0bk_tools','2bk_body','2bk_faces','2bk_places','2bk_tools']},\n",
        "    'EMOTION'    : {'cond':['fear','neut']},\n",
        "    'GAMBLING'   : {'cond':['loss','win']},\n",
        "    'LANGUAGE'   : {'cond':['math','story']},\n",
        "    'RELATIONAL' : {'cond':['match','relation']},\n",
        "    'SOCIAL'     : {'cond':['ment','rnd']}\n",
        "}"
      ],
      "metadata": {
        "id": "IdidiIdR00Yp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading data"
      ],
      "metadata": {
        "id": "szb0d-fS1imT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download data file\n",
        "import os, requests\n",
        "\n",
        "fname = \"hcp_task.tgz\"\n",
        "url = \"https://osf.io/2y3fw/download\"\n",
        "\n",
        "if not os.path.isfile(fname):\n",
        "  try:\n",
        "    r = requests.get(url)\n",
        "  except requests.ConnectionError:\n",
        "    print(\"!!! Failed to download data !!!\")\n",
        "  else:\n",
        "    if r.status_code != requests.codes.ok:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      with open(fname, \"wb\") as fid:\n",
        "        fid.write(r.content)"
      ],
      "metadata": {
        "id": "JUvfM5ra1lcB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The download cells will store the data in nested directories starting here:\n",
        "HCP_DIR = \"./hcp_task\"\n",
        "\n",
        "# importing the \"tarfile\" module\n",
        "import tarfile\n",
        "\n",
        "# open file\n",
        "with tarfile.open(fname) as tfile:\n",
        "  # extracting file\n",
        "  tfile.extractall('.')\n",
        "\n",
        "subjects = np.loadtxt(os.path.join(HCP_DIR, 'subjects_list.txt'), dtype='str')"
      ],
      "metadata": {
        "id": "JxhpLHRo1s04"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_single_timeseries(subject, experiment, run, remove_mean=True):\n",
        "  \"\"\"Load timeseries data for a single subject and single run.\n",
        "\n",
        "  Args:\n",
        "    subject (str):      subject ID to load\n",
        "    experiment (str):   Name of experiment\n",
        "    run (int):          (0 or 1)\n",
        "    remove_mean (bool): If True, subtract the parcel-wise mean (typically the mean BOLD signal is not of interest)\n",
        "\n",
        "  Returns\n",
        "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
        "\n",
        "  \"\"\"\n",
        "  bold_run  = RUNS[run]\n",
        "  bold_path = f\"{HCP_DIR}/subjects/{subject}/{experiment}/tfMRI_{experiment}_{bold_run}\"\n",
        "  bold_file = \"data.npy\"\n",
        "  ts = np.load(f\"{bold_path}/{bold_file}\")\n",
        "  if remove_mean:\n",
        "    ts -= ts.mean(axis=1, keepdims=True)\n",
        "  return ts"
      ],
      "metadata": {
        "id": "PYYjWDGA2_0m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to concatenate language data (2 runs, 316 frames)"
      ],
      "metadata": {
        "id": "5EsNjR4sG5YI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "def concatenate_task_data():\n",
        "   \"\"\"\n",
        "   Concatenate data.npy files for all subjects for each task.\n",
        "   For each task, concatenate LR and RL runs for each subject, then stack across subjects.\n",
        "\n",
        "   Returns:\n",
        "       task_data (dict): Dictionary mapping task names to arrays of shape (n_subjects, n_parcels, n_timepoints_total)\n",
        "   \"\"\"\n",
        "   task_data = {task: [] for task in EXPERIMENTS}\n",
        "\n",
        "   # Iterate through subjects\n",
        "   for subject in subjects:\n",
        "       subject_folder = Path(HCP_DIR) / \"subjects\" / subject\n",
        "       if not subject_folder.is_dir():\n",
        "           print(f\"Subject directory {subject_folder} does not exist. Skipping.\")\n",
        "           continue\n",
        "\n",
        "       # Iterate through tasks\n",
        "       for task in EXPERIMENTS:\n",
        "           task_folder = subject_folder / task\n",
        "           if not task_folder.is_dir():\n",
        "               print(f\"Task directory {task_folder} does not exist for subject {subject}. Skipping.\")\n",
        "               continue\n",
        "\n",
        "           # Load and concatenate LR and RL runs\n",
        "           subject_task_data = []\n",
        "           for run_idx, run_suffix in enumerate(RUNS):\n",
        "               try:\n",
        "                   ts = load_single_timeseries(subject, task, run_idx, remove_mean=False)\n",
        "                   subject_task_data.append(ts)\n",
        "               except Exception as e:\n",
        "                   print(f\"Error loading data.npy for {subject}/{task}/tfMRI_{task}_{run_suffix}: {e}\")\n",
        "                   subject_task_data.append(None)\n",
        "\n",
        "           # Check if any run failed to load\n",
        "           if any(x is None for x in subject_task_data):\n",
        "               print(f\"Skipping subject {subject} for task {task} due to missing run data.\")\n",
        "               continue\n",
        "\n",
        "           # Concatenate LR and RL runs along the time axis (axis=1)\n",
        "           try:\n",
        "               concatenated_ts = np.concatenate(subject_task_data, axis=1)\n",
        "               task_data[task].append(concatenated_ts)\n",
        "           except Exception as e:\n",
        "               print(f\"Error concatenating runs for {subject}/{task}: {e}\")\n",
        "               continue\n",
        "\n",
        "   # Stack subject arrays for each task\n",
        "   result = {}\n",
        "   for task in task_data:\n",
        "       if not task_data[task]:\n",
        "           print(f\"No data concatenated for task {task}.\")\n",
        "           continue\n",
        "       try:\n",
        "           # Stack along a new axis (0) to get (n_subjects, n_parcels, n_timepoints_total)\n",
        "           task_array = np.stack(task_data[task], axis=0)\n",
        "           result[task] = task_array\n",
        "           print(f\"Task {task}: Concatenated array shape {task_array.shape}\")\n",
        "       except Exception as e:\n",
        "           print(f\"Error stacking subjects for task {task}: {e}\")\n",
        "\n",
        "   return result\n",
        "\n",
        "task_data = concatenate_task_data()\n",
        "language_data = task_data[\"LANGUAGE\"]"
      ],
      "metadata": {
        "id": "CeqgZxe3HDOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db9634b1-b4d6-43f3-b251-4c22371b37c1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task MOTOR: Concatenated array shape (100, 360, 568)\n",
            "Task WM: Concatenated array shape (100, 360, 810)\n",
            "Task EMOTION: Concatenated array shape (100, 360, 352)\n",
            "Task GAMBLING: Concatenated array shape (100, 360, 506)\n",
            "Task LANGUAGE: Concatenated array shape (100, 360, 632)\n",
            "Task RELATIONAL: Concatenated array shape (100, 360, 464)\n",
            "Task SOCIAL: Concatenated array shape (100, 360, 548)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print list of ROIs\n",
        "\n",
        "filename = os.path.join(HCP_DIR, 'MMP_parcellation_updated.txt')\n",
        "roi_info = pd.read_csv(filename, sep='\\t')\n",
        "roi_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "5FXnUrKd36b3",
        "outputId": "1776acb8-9414-469a-c52e-6a265b7fa5c4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './hcp_task/MMP_parcellation_updated.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-2055968092.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHCP_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MMP_parcellation_updated.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mroi_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mroi_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './hcp_task/MMP_parcellation_updated.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# === STEP 1: Load Excel ===\n",
        "df = pd.read_excel('Glasser_2016_Table.xlsx', sheet_name='Sheet1', skiprows=1)\n",
        "# print(df.columns.tolist())\n",
        "\n",
        "\n",
        "# Replace with your actual column name\n",
        "roi_column = 'Area\\xa0Description'\n",
        "\n",
        "# === STEP 2: Brain Regions from Literature ===\n",
        "literature_regions = [\n",
        "    \"superior temporal\",       # catches STG, posterior STG\n",
        "    \"supramarginal\",\n",
        "    \"wernicke\",\n",
        "    \"middle temporal\",         # MTG\n",
        "    \"inferior temporal\",       # ITG\n",
        "    \"angular\",                 # angular gyrus\n",
        "    \"pars orbitalis\",          # IFGpo\n",
        "    \"inferior frontal\",        # catches IFG as well\n",
        "    \"dorsomedial prefrontal\",\n",
        "    \"pars triangularis\",       # IFGpt\n",
        "    \"pars opercularis\",        # IFGop\n",
        "    \"inferior frontal sulcus\", # IFS\n",
        "    \"broca\",\n",
        "    \"sensorimotor\",\n",
        "    \"visual word\"\n",
        "]\n",
        "\n",
        "# === STEP 3: Matching Function ===\n",
        "def match_region(roi_name, region_list):\n",
        "    roi_name = str(roi_name).lower()\n",
        "    return any(region.lower() in roi_name for region in region_list)\n",
        "\n",
        "# === STEP 4: Filter Matching ROIs ===\n",
        "matched_df = df[df[roi_column].apply(lambda x: match_region(x, literature_regions))]\n",
        "\n",
        "# === STEP 5: Output ===\n",
        "print(\"Matched ROIs:\")\n",
        "print(matched_df)\n",
        "\n",
        "# Save if needed\n",
        "matched_df.to_excel(\"matched_rois_from_literature.xlsx\", index=False)\n"
      ],
      "metadata": {
        "id": "Ou-o4Du-KJN8",
        "outputId": "864a53d0-0761-4675-aad3-b31a49695648",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Parcel\\nIndex', 'Area\\nName', 'Area\\xa0Description', 'New?', 'Sections', 'Other\\xa0Names', 'Key\\xa0Studies']\n",
            "Matched ROIs:\n",
            "    Parcel\\nIndex Area\\nName                Area Description New?  Sections  \\\n",
            "22             23         MT           Middle Temporal\\nArea   No      5,15   \n",
            "27             28        STV  Superior Temporal\\nVisual Area  Yes  11,15,17   \n",
            "\n",
            "    Other Names                                        Key Studies  \n",
            "22  hOC5, hOC5d  Abdollahi et al 2014, Kolster et al 2010, Mali...  \n",
            "27          NaN                                                NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# === STEP 1: Load Excel and clean headers ===\n",
        "df = pd.read_excel('Glasser_2016_Table.xlsx', sheet_name='Sheet1', skiprows=1)\n",
        "\n",
        "# Clean column names: remove leading/trailing spaces and \\n\n",
        "df.columns = df.columns.str.strip().str.replace('\\n', ' ', regex=True)\n",
        "\n",
        "# Clean \"Area Description\" entries\n",
        "df['Area\\xa0Description'] = df['Area\\xa0Description'].astype(str).str.strip().str.replace('\\n', ' ', regex=True)\n",
        "\n",
        "# Define the column to match on\n",
        "roi_column = 'Area\\xa0Description'\n",
        "\n",
        "# === STEP 2: Brain Regions from Literature ===\n",
        "literature_regions = [\n",
        "    # Full names\n",
        "    \"superior temporal\", \"posterior superior temporal\", \"supramarginal\",\n",
        "    \"wernicke\", \"middle temporal\", \"inferior temporal\", \"angular\",\n",
        "    \"pars orbitalis\", \"inferior frontal\", \"dorsomedial prefrontal\",\n",
        "    \"pars triangularis\", \"pars opercularis\", \"inferior frontal sulcus\",\n",
        "    \"broca\", \"sensorimotor\", \"visual word\",\n",
        "\n",
        "    # Common abbreviations\n",
        "    \"STG\", \"MTG\", \"ITG\", \"IFGpo\", \"IFGpt\", \"IFGop\", \"SMG\", \"AG\", \"IFS\", \"DMPFC\", \"VWFA\"\n",
        "]\n",
        "\n",
        "# === STEP 3: Matching Function (case-insensitive substring match) ===\n",
        "def match_region(roi_name, region_list):\n",
        "    roi_name = str(roi_name).lower()\n",
        "    return any(region.lower() in roi_name for region in region_list)\n",
        "\n",
        "# === STEP 4: Filter Matching ROIs ===\n",
        "matched_df = df[df[roi_column].apply(lambda x: match_region(x, literature_regions))]\n",
        "\n",
        "# === STEP 5: Output Results ===\n",
        "print(\"Matched ROIs:\")\n",
        "print(matched_df[[roi_column, 'Other\\xa0Names', 'Key\\xa0Studies']])\n",
        "\n",
        "# Save cleaned matched ROIs\n",
        "matched_df.to_excel(\"matched_rois_from_literature_cleaned.xlsx\", index=False)\n"
      ],
      "metadata": {
        "id": "iqOHZkTYMfnD",
        "outputId": "364d46f1-bdf5-4c13-c136-c92817b03124",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matched ROIs:\n",
            "                      Area Description        Other Names  \\\n",
            "1        Medial Superior Temporal Area  MSTv, hOC5, hOC5v   \n",
            "22                Middle Temporal Area        hOC5, hOC5d   \n",
            "24           PeriSylvian Language Area                NaN   \n",
            "25      Superior Frontal Language Area                NaN   \n",
            "27       Superior Temporal Visual Area                NaN   \n",
            "80                           Area IFSp                NaN   \n",
            "81                           Area IFSa                NaN   \n",
            "111  Anterior Agranular Insula Complex           Iai, Ial   \n",
            "122                          Area STGa                NaN   \n",
            "\n",
            "                                           Key Studies  \n",
            "1    Abdollahi et al 2014, Kolster et al 2010, Mali...  \n",
            "22   Abdollahi et al 2014, Kolster et al 2010, Mali...  \n",
            "24                                                 NaN  \n",
            "25                                                 NaN  \n",
            "27                                                 NaN  \n",
            "80                                                 NaN  \n",
            "81                                                 NaN  \n",
            "111            Van Essen et al 2012b, Ongur et al 2003  \n",
            "122                                                NaN  \n"
          ]
        }
      ]
    }
  ]
}